{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (1.1.2)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in d:\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-docx pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4320e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in d:\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Using cached h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Using cached hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Using cached hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: hyperframe, hpack, h11, chardet, idna, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: hyperframe\n",
      "    Found existing installation: hyperframe 6.1.0\n",
      "    Uninstalling hyperframe-6.1.0:\n",
      "      Successfully uninstalled hyperframe-6.1.0\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.1.0\n",
      "    Uninstalling hpack-4.1.0:\n",
      "      Successfully uninstalled hpack-4.1.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.2.0\n",
      "    Uninstalling chardet-5.2.0:\n",
      "      Successfully uninstalled chardet-5.2.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.2.0\n",
      "    Uninstalling h2-4.2.0:\n",
      "      Successfully uninstalled h2-4.2.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 4.0.2\n",
      "    Uninstalling googletrans-4.0.2:\n",
      "      Successfully uninstalled googletrans-4.0.2\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script chardetect.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai 1.90.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "jupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb88f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 4 个文件\n",
      "✓ 已处理: Chapter 1 (56 段落)\n",
      "✓ 已处理: Chapter 2 (4 段落)\n",
      "✓ 已处理: Chapter 3 (64 段落)\n",
      "✓ 已处理: Chapter 4 (146 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\Red sorghum_Howard Goldblatt\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Howard Goldblatt\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"Red sorghum_Howard Goldblatt_Chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_Sorghum_Chapter{chapter}_Howard_Goldblatt\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_sorghum_Chapter{chapter}_Howard_Goldblatt.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0882aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 4 个文件\n",
      "✓ 已处理: Chapter 1 (44 段落)\n",
      "✓ 已处理: Chapter 2 (5 段落)\n",
      "✓ 已处理: Chapter 3 (63 段落)\n",
      "✓ 已处理: Chapter 4 (143 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\Red_sorghum_expert_merge\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Red_sorghum_expert_merge\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"Red sorghum_Howard Goldblatt_Chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_Sorghum_Chapter{chapter}_Howard_Goldblatt\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_sorghum_Chapter{chapter}_Howard_Goldblatt.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87799245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 3 个文件\n",
      "✓ 已处理: Chapter 1 (127 段落)\n",
      "✓ 已处理: Chapter 2 (77 段落)\n",
      "✓ 已处理: Chapter 3 (178 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\A Dream of Red Mansions_H.Bencraft Joly\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\H.Bencraft Joly\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"A Dream of Red Mansions_H.Bencraft Joly_Chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_Mansions_Chapter{chapter}_Bencraft_Joly\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_Mansions_Chapter{chapter}_Bencraft_Joly.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abbc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 3 个文件\n",
      "✓ 已处理: Chapter 1 (61 段落)\n",
      "✓ 已处理: Chapter 2 (23 段落)\n",
      "✓ 已处理: Chapter 3 (43 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\Red_mansion_expert_merge\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Red_mansion_expert_merge\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"A Dream of Red Mansions_H.Bencraft Joly_Chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_Mansions_Chapter{chapter}_Bencraft_Joly\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_Mansions_Chapter{chapter}_Bencraft_Joly.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bba95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 3 个文件\n",
      "✓ 已处理: Chapter 1 (47 段落)\n",
      "✓ 已处理: Chapter 2 (24 段落)\n",
      "✓ 已处理: Chapter 3 (40 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\Chinese_A Dream of Red Mansions\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Chinese_A Dream of Red Mansions\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"Chinese_A Dream of Red Mansions_chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_Mansions_Chapter{chapter}_Chinese\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_Mansions_Chapter{chapter}_Chinese.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb10792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 4 个文件\n",
      "✓ 已处理: Chapter 1 (43 段落)\n",
      "✓ 已处理: Chapter 2 (5 段落)\n",
      "✓ 已处理: Chapter 3 (63 段落)\n",
      "✓ 已处理: Chapter 4 (143 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\Chinese_Red sorghum\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Chinese_Red sorghum\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"Chinese_Red sorghum_Chapter*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Red_sorghum_Chapter{chapter}_Chinese\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Red_sorghum_Chapter{chapter}_Chinese.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: Chapter {chapter} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193afac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 8 个文件\n",
      "✓ 已处理: News 1 (7 段落)\n",
      "✓ 已处理: News 2 (17 段落)\n",
      "✓ 已处理: News 3 (8 段落)\n",
      "✓ 已处理: News 4 (10 段落)\n",
      "✓ 已处理: News 5 (9 段落)\n",
      "✓ 已处理: News 6 (9 段落)\n",
      "✓ 已处理: News 7 (23 段落)\n",
      "✓ 已处理: News 8 (7 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\News_English_Version\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\News_English_Version\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"English_version_news*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    news = file_name.split(\"news\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"English_version_news{news}\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"English_version_news{news}.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: News {news} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb052006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 8 个文件\n",
      "✓ 已处理: News 1 (7 段落)\n",
      "✓ 已处理: News 2 (7 段落)\n",
      "✓ 已处理: News 3 (7 段落)\n",
      "✓ 已处理: News 4 (7 段落)\n",
      "✓ 已处理: News 5 (8 段落)\n",
      "✓ 已处理: News 6 (8 段落)\n",
      "✓ 已处理: News 7 (21 段落)\n",
      "✓ 已处理: News 8 (6 段落)\n",
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import os\n",
    "import glob\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\Select matreial\\News_Chinese_Version\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\News_Chinese_Version\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 查找所有章节文件\n",
    "docx_files = glob.glob(os.path.join(input_folder, \"Chinese_version_news*.docx\"))\n",
    "\n",
    "print(f\"找到 {len(docx_files)} 个文件\")\n",
    "\n",
    "# 处理每个文件\n",
    "for docx_file in sorted(docx_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(docx_file)\n",
    "    news = file_name.split(\"news\")[1].split(\".\")[0]\n",
    "    \n",
    "    # 读取docx\n",
    "    doc = Document(docx_file)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(paragraphs, columns=[f\"Chinese_version_news{news}\"])\n",
    "    \n",
    "    # 保存CSV\n",
    "    csv_name = f\"Chinese_version_news{news}.csv\"\n",
    "    df.to_csv(os.path.join(output_folder, csv_name), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 已处理: News {news} ({len(paragraphs)} 段落)\")\n",
    "\n",
    "print(\"完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d253ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 3 个文件待翻译\n",
      "\n",
      "正在处理: Chapter 1\n",
      "  已翻译: 10/47 段落\n",
      "  已翻译: 20/47 段落\n",
      "  已翻译: 30/47 段落\n",
      "  已翻译: 40/47 段落\n",
      "✓ 完成: Chapter 1\n",
      "\n",
      "正在处理: Chapter 2\n",
      "  已翻译: 10/24 段落\n",
      "  已翻译: 20/24 段落\n",
      "✓ 完成: Chapter 2\n",
      "\n",
      "正在处理: Chapter 3\n",
      "  已翻译: 10/40 段落\n",
      "  已翻译: 20/40 段落\n",
      "  已翻译: 30/40 段落\n",
      "  已翻译: 40/40 段落\n",
      "✓ 完成: Chapter 3\n",
      "\n",
      "所有翻译完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Chinese_A Dream of Red Mansions\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\google_Red Mansions\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 创建翻译器\n",
    "translator = Translator()\n",
    "\n",
    "# 查找所有中文CSV文件\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"Red_Mansions_Chapter*_Chinese.csv\"))\n",
    "\n",
    "print(f\"找到 {len(csv_files)} 个文件待翻译\")\n",
    "\n",
    "# 处理每个文件\n",
    "for csv_file in sorted(csv_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(csv_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\"_\")[0]\n",
    "    \n",
    "    print(f\"\\n正在处理: Chapter {chapter}\")\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    \n",
    "    # 获取列名和内容\n",
    "    column_name = df.columns[0]\n",
    "    chinese_texts = df[column_name].tolist()\n",
    "    \n",
    "    # 翻译每个段落\n",
    "    english_texts = []\n",
    "    for i, text in enumerate(chinese_texts):\n",
    "        try:\n",
    "            # 翻译文本\n",
    "            translated = translator.translate(text, src='zh-cn', dest='en').text\n",
    "            english_texts.append(translated)\n",
    "            \n",
    "            # 显示进度\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  已翻译: {i + 1}/{len(chinese_texts)} 段落\")\n",
    "            \n",
    "            # 延迟避免请求过快\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  翻译错误 (段落 {i+1}): {str(e)}\")\n",
    "            english_texts.append(text)  # 保留原文\n",
    "    \n",
    "    # 创建新的DataFrame\n",
    "    new_column_name = f\"Red_Mansions_Chapter{chapter}_Google\"\n",
    "    df_english = pd.DataFrame(english_texts, columns=[new_column_name])\n",
    "    \n",
    "    # 保存翻译结果\n",
    "    output_file = f\"Red_Mansions_Chapter{chapter}_Google.csv\"\n",
    "    df_english.to_csv(os.path.join(output_folder, output_file), \n",
    "                      index=False, \n",
    "                      encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 完成: Chapter {chapter}\")\n",
    "\n",
    "print(\"\\n所有翻译完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4072110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 4 个文件待翻译\n",
      "\n",
      "正在处理: Chapter 1\n",
      "  已翻译: 10/43 段落\n",
      "  已翻译: 20/43 段落\n",
      "  已翻译: 30/43 段落\n",
      "  已翻译: 40/43 段落\n",
      "✓ 完成: Chapter 1\n",
      "\n",
      "正在处理: Chapter 2\n",
      "✓ 完成: Chapter 2\n",
      "\n",
      "正在处理: Chapter 3\n",
      "  已翻译: 10/63 段落\n",
      "  已翻译: 20/63 段落\n",
      "  已翻译: 30/63 段落\n",
      "  已翻译: 40/63 段落\n",
      "  已翻译: 50/63 段落\n",
      "  已翻译: 60/63 段落\n",
      "✓ 完成: Chapter 3\n",
      "\n",
      "正在处理: Chapter 4\n",
      "  已翻译: 10/143 段落\n",
      "  已翻译: 20/143 段落\n",
      "  已翻译: 30/143 段落\n",
      "  已翻译: 40/143 段落\n",
      "  已翻译: 50/143 段落\n",
      "  已翻译: 60/143 段落\n",
      "  已翻译: 70/143 段落\n",
      "  已翻译: 80/143 段落\n",
      "  已翻译: 90/143 段落\n",
      "  已翻译: 100/143 段落\n",
      "  已翻译: 110/143 段落\n",
      "  已翻译: 120/143 段落\n",
      "  已翻译: 130/143 段落\n",
      "  已翻译: 140/143 段落\n",
      "✓ 完成: Chapter 4\n",
      "\n",
      "所有翻译完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\Chinese_Red sorghum\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\google_Red sorghum\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 创建翻译器\n",
    "translator = Translator()\n",
    "\n",
    "# 查找所有中文CSV文件\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"Red_sorghum_Chapter*_Chinese.csv\"))\n",
    "\n",
    "print(f\"找到 {len(csv_files)} 个文件待翻译\")\n",
    "\n",
    "# 处理每个文件\n",
    "for csv_file in sorted(csv_files):\n",
    "    # 获取章节号\n",
    "    file_name = os.path.basename(csv_file)\n",
    "    chapter = file_name.split(\"Chapter\")[1].split(\"_\")[0]\n",
    "    \n",
    "    print(f\"\\n正在处理: Chapter {chapter}\")\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    \n",
    "    # 获取列名和内容\n",
    "    column_name = df.columns[0]\n",
    "    chinese_texts = df[column_name].tolist()\n",
    "    \n",
    "    # 翻译每个段落\n",
    "    english_texts = []\n",
    "    for i, text in enumerate(chinese_texts):\n",
    "        try:\n",
    "            # 翻译文本\n",
    "            translated = translator.translate(text, src='zh-cn', dest='en').text\n",
    "            english_texts.append(translated)\n",
    "            \n",
    "            # 显示进度\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  已翻译: {i + 1}/{len(chinese_texts)} 段落\")\n",
    "            \n",
    "            # 延迟避免请求过快\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  翻译错误 (段落 {i+1}): {str(e)}\")\n",
    "            english_texts.append(text)  # 保留原文\n",
    "    \n",
    "    # 创建新的DataFrame\n",
    "    new_column_name = f\"Red_sorghum_Chapter{chapter}_Google\"\n",
    "    df_english = pd.DataFrame(english_texts, columns=[new_column_name])\n",
    "    \n",
    "    # 保存翻译结果\n",
    "    output_file = f\"Red_sorghum_Chapter{chapter}_Google.csv\"\n",
    "    df_english.to_csv(os.path.join(output_folder, output_file), \n",
    "                      index=False, \n",
    "                      encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 完成: Chapter {chapter}\")\n",
    "\n",
    "print(\"\\n所有翻译完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e328b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 8 个文件待翻译\n",
      "\n",
      "正在处理: News 1\n",
      "✓ 完成: News 1\n",
      "\n",
      "正在处理: News 2\n",
      "✓ 完成: News 2\n",
      "\n",
      "正在处理: News 3\n",
      "✓ 完成: News 3\n",
      "\n",
      "正在处理: News 4\n",
      "✓ 完成: News 4\n",
      "\n",
      "正在处理: News 5\n",
      "✓ 完成: News 5\n",
      "\n",
      "正在处理: News 6\n",
      "✓ 完成: News 6\n",
      "\n",
      "正在处理: News 7\n",
      "  已翻译: 10/21 段落\n",
      "  已翻译: 20/21 段落\n",
      "✓ 完成: News 7\n",
      "\n",
      "正在处理: News 8\n",
      "✓ 完成: News 8\n",
      "\n",
      "所有翻译完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# 设置路径\n",
    "input_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\News_Chinese_Version\"\n",
    "output_folder = r\"C:\\Users\\ASUS\\Desktop\\LLM translate\\Data_set\\google_news\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 创建翻译器\n",
    "translator = Translator()\n",
    "\n",
    "# 查找所有中文新闻CSV文件\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"Chinese_version_news*.csv\"))\n",
    "\n",
    "print(f\"找到 {len(csv_files)} 个文件待翻译\")\n",
    "\n",
    "# 处理每个文件\n",
    "for csv_file in sorted(csv_files):\n",
    "    # 获取文件编号\n",
    "    file_name = os.path.basename(csv_file)\n",
    "    # 提取数字部分 (Chinese_version_news1.csv -> 1)\n",
    "    news_number = file_name.replace(\"Chinese_version_news\", \"\").replace(\".csv\", \"\")\n",
    "    \n",
    "    print(f\"\\n正在处理: News {news_number}\")\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    \n",
    "    # 获取列名和内容\n",
    "    column_name = df.columns[0]\n",
    "    chinese_texts = df[column_name].tolist()\n",
    "    \n",
    "    # 翻译每个段落\n",
    "    english_texts = []\n",
    "    for i, text in enumerate(chinese_texts):\n",
    "        try:\n",
    "            # 翻译文本\n",
    "            translated = translator.translate(text, src='zh-cn', dest='en').text\n",
    "            english_texts.append(translated)\n",
    "            \n",
    "            # 显示进度\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  已翻译: {i + 1}/{len(chinese_texts)} 段落\")\n",
    "            \n",
    "            # 延迟避免请求过快\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  翻译错误 (段落 {i+1}): {str(e)}\")\n",
    "            english_texts.append(text)  # 保留原文\n",
    "    \n",
    "    # 创建新的DataFrame\n",
    "    new_column_name = f\"Google_news{news_number}\"\n",
    "    df_english = pd.DataFrame(english_texts, columns=[new_column_name])\n",
    "    \n",
    "    # 保存翻译结果\n",
    "    output_file = f\"Google_news{news_number}.csv\"\n",
    "    df_english.to_csv(os.path.join(output_folder, output_file), \n",
    "                      index=False, \n",
    "                      encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"✓ 完成: News {news_number}\")\n",
    "\n",
    "print(\"\\n所有翻译完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
